{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from decision import haversine\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta, date\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('1_250_full_data.csv',\n",
    "                        index_col=0,\n",
    "                        #usecols = ['POLYLINE', 'Destination'],\n",
    "                        converters={'POLYLINE': lambda x: json.loads(x), 'Destination': lambda x: json.loads(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traj_score(targets, predicted):\n",
    "    return np.mean(np.power([haversine(targets[i], predicted[i]) for i in xrange(len(targets))], 2))\n",
    "\n",
    "def data_processing_to_train(data, get_dummies=True, y_in_list=True):\n",
    "    #this data also can used for clustering (before creat dummies)\n",
    "    \n",
    "    # make y for predictions\n",
    "    if y_in_list == True:\n",
    "        y = list(np.array(data['Destination']))\n",
    "    else:\n",
    "        y = np.array(data['Destination'])\n",
    "    \n",
    "    # add start_lat and start_lon to the dataset\n",
    "    traj = np.array(data['POLYLINE'])\n",
    "    data['Start_lon'] = [i[0][0] for i in traj]\n",
    "    data['Start_lat'] = [i[0][1] for i in traj]\n",
    "    \n",
    "    # fill NA values as specific class\n",
    "    data = data.fillna('0')\n",
    "    \n",
    "    # make some columns categorical (str)\n",
    "    cat_columns = ['CALL_TYPE','DAY_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID']\n",
    "    data[cat_columns] = data[cat_columns].astype(str)\n",
    "    \n",
    "    # drop irrelevant columns\n",
    "    data = data.drop(['Destination','MISSING_DATA', 'TRIP_ID', 'POLYLINE', 'DAY_TYPE'], 1)\n",
    "    \n",
    "    # next step is to work with timestamps\n",
    "    \n",
    "    # get list of holidays in Portugal\n",
    "    pd_holidays = pd.read_csv('Portugal_holidays.csv')\n",
    "    holidays = [date(pd_holidays.Year[i], pd_holidays.Month[i], pd_holidays.Day[i]) for i in range(len(pd_holidays))]\n",
    "    del pd_holidays\n",
    "    \n",
    "    # get year, month, day, seconds from midnight, weeknumber, weekday\n",
    "\n",
    "    dates = []\n",
    "    for tmstmp in data['TIMESTAMP']:\n",
    "        t = datetime.utcfromtimestamp(tmstmp)\n",
    "        date_info = []\n",
    "        date_info.append(t.year) # year\n",
    "        date_info.append(t.month) # month\n",
    "        date_info.append(t.isocalendar()[1]) # weeknumber\n",
    "        date_info.append(t.weekday()) # weekday where Monday is 0 and Sunday is 6\n",
    "        date_info.append((t.hour * 3600) + (t.minute * 60) + t.second + (t.microsecond / 1000000.0)) # seconds from midnight\n",
    "\n",
    "        t_date = t.date()\n",
    "        if t_date in holidays:\n",
    "            date_info.append('B')\n",
    "        elif (date_info[3] == 6) or (date_info[3] == 5):\n",
    "            date_info.append('B')\n",
    "        elif (t_date + timedelta(1)) in holidays:\n",
    "            date_info.append('C')\n",
    "        elif date_info[3] == 4:\n",
    "            date_info.append('C')\n",
    "        else:\n",
    "            date_info.append('A')\n",
    "\n",
    "        dates.append(date_info)\n",
    "\n",
    "    dates = np.array(dates)\n",
    "    dates_columns = ['YEAR', 'MONTH', 'WEEKNUM', 'WEEKDAY', 'SECFROMMID', 'DAY_TYPE']\n",
    "    dates_df = pd.DataFrame(data = dates, columns = dates_columns)\n",
    "\n",
    "    data = pd.concat([data, dates_df], axis=1)\n",
    "    data['SECFROMMID'] = data['SECFROMMID'].astype(float)\n",
    "    \n",
    "    if get_dummies == True:\n",
    "        dummy_cols = ['CALL_TYPE','DAY_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID', \n",
    "              'YEAR', 'MONTH', 'WEEKNUM', 'WEEKDAY']\n",
    "\n",
    "        data = pd.get_dummies(data, columns=dummy_cols)\n",
    "\n",
    "        drop_columns = ['CALL_TYPE_A', 'DAY_TYPE_A', 'ORIGIN_CALL_2002.0', 'ORIGIN_STAND_1.0', 'TAXI_ID_20000066.0',\n",
    "                        'YEAR_2013', 'MONTH_1', 'WEEKNUM_1', 'WEEKDAY_0']\n",
    "        data = data.drop(drop_columns,1)\n",
    "        \n",
    "        return data, y\n",
    "    else:\n",
    "        return data, y\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = data_processing_to_train(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only best models will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6914939679\n",
      "18.2345812708\n",
      "18.1697651787\n",
      "18.6459230394\n",
      "18.7016556094\n"
     ]
    }
   ],
   "source": [
    "min_samples_leaf = len(X_train)/32\n",
    "\n",
    "# kNN\n",
    "kNN_regr = MultiOutputRegressor(KNeighborsRegressor(59))\n",
    "kNN_regr.fit(X_train, y_train)\n",
    "print traj_score(y_test, kNN_regr.predict(X_test))\n",
    "\n",
    "# CART\n",
    "CART_regr = MultiOutputRegressor(DecisionTreeRegressor(max_depth=3,\n",
    "                                                       random_state=0,\n",
    "                                                       min_samples_leaf=min_samples_leaf))\n",
    "CART_regr.fit(X_train, y_train)\n",
    "print traj_score(y_test, CART_regr.predict(X_test))\n",
    "\n",
    "# RF\n",
    "RF_regr = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,\n",
    "                                                     max_depth=3,\n",
    "                                                     random_state=0,\n",
    "                                                     min_samples_leaf=min_samples_leaf))\n",
    "RF_regr.fit(X_train, y_train)\n",
    "print traj_score(y_test, RF_regr.predict(X_test))\n",
    "\n",
    "# AdaBoost\n",
    "Ada_regr = MultiOutputRegressor(AdaBoostRegressor(n_estimators=10, \n",
    "                                                 base_estimator=(DecisionTreeRegressor(max_depth=3)),\n",
    "                                                 random_state=0,\n",
    "                                                 learning_rate = 0.1))\n",
    "Ada_regr.fit(X_train, y_train)\n",
    "print traj_score(y_test, Ada_regr.predict(X_test))\n",
    "\n",
    "# XgBoost\n",
    "XG_regr = MultiOutputRegressor(xg.XGBRegressor(n_estimators=1000,\n",
    "                                               max_depth=1,\n",
    "                                               learning_rate=0.1,\n",
    "                                               seed=0))\n",
    "XG_regr.fit(X_train, y_train)\n",
    "print traj_score(y_test, XG_regr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stacking(models, train):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        predictions.append(model.predict(train))\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajectories = list(data['POLYLINE'])\n",
    "trunc_trajectories = [traj[:random.randint(1, len(traj))] for traj in trajectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = pickle.load( open( \"forest_250_data_10_trees.p\", \"rb\" ) )\n",
    "tree = pickle.load( open( \"tree_250_data.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [kNN_regr, CART_regr, RF_regr, Ada_regr, XG_regr]\n",
    "stack_X = stacking(models, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stack_X.append(np.array(forest.predict(trunc_trajectories)))\n",
    "stack_X.append(np.array(tree.predict(trunc_trajectories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stack_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_stack_X = (np.array(stack_X[:6]))\n",
    "#print norm_stack_X.shape\n",
    "norm_stack_X = np.transpose(norm_stack_X, axes=(1,0,2))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(norm_stack_X, y, test_size=0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.61899141,  41.16067246],\n",
       "       [ -8.61672136,  41.15295752],\n",
       "       [ -8.61749112,  41.1576789 ],\n",
       "       [ -8.61151699,  41.16065157],\n",
       "       [ -8.61255169,  41.16029739],\n",
       "       [ -8.62074079,  41.16623177]])"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_stack_X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a prediction with coefficients\n",
    "def sgd_predict_one(coef, row):\n",
    "    yhat = coef[0]\n",
    "    for i in range(len(row)):\n",
    "        yhat = yhat + coef[i + 1] * row[i]\n",
    "    return yhat\n",
    " \n",
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(X_train, y_train, l_rate=0.01, n_epoch=5000):\n",
    "    #coef = [1./(len(X_train[0]))]*len(X_train[0])\n",
    "    coef = [np.array([1./(len(X_train[0])), 1./(len(X_train[0]))]) for i in range(len(X_train[0]) + 1)]\n",
    "    print coef\n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range(len(X_train)):\n",
    "            yhat = sgd_predict_one(coef, X_train[i])\n",
    "            #error = haversine(y_train[i], yhat)\n",
    "            error = y_train[i] - yhat\n",
    "            #print 'error', error\n",
    "            #print 'test', coef\n",
    "            coef[0] = coef[0] - l_rate * error\n",
    "            #print 'coef[0]', coef[0]\n",
    "            for j in range(len(coef) - 1):\n",
    "                #coef[j + 1] = coef[j + 1] - l_rate * X_train[i][j] * error\n",
    "                coef[j + 1] = coef[j + 1] - l_rate * X_train[i][j] * error\n",
    "            #print coef\n",
    "    return coef    \n",
    "\n",
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "def sgd_predict(X_test, coef):\n",
    "    return [sgd_predict_one(coef, row) for row in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.2,  0.2]), array([ 0.2,  0.2]), array([ 0.2,  0.2]), array([ 0.2,  0.2]), array([ 0.2,  0.2]), array([ 0.2,  0.2])]\n",
      "[array([ 0.200257  ,  0.21078196]), array([ 0.19778463,  0.64379377]), array([ 0.19778423,  0.64379256]), array([ 0.19778425,  0.64382019]), array([ 0.1977847 ,  0.64380248]), array([ 0.19778424,  0.64376752])]\n"
     ]
    }
   ],
   "source": [
    "#del sgd_coef\n",
    "sgd_coef = coefficients_sgd(X_train[:10], y_train[:10], n_epoch=1, l_rate=0.0001)\n",
    "print sgd_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.49816307126e+37\n"
     ]
    }
   ],
   "source": [
    "print traj_score(y_test, sgd_predict(X_test, sgd_coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6511339747\n"
     ]
    }
   ],
   "source": [
    "stacked_lons_train = np.transpose(np.array(X_train), axes=(2,0,1))[0]\n",
    "stacked_lats_train = np.transpose(np.array(X_train), axes=(2,0,1))[1]\n",
    "\n",
    "lons_train = np.array(y_train)[:,0]\n",
    "lats_train = np.array(y_train)[:,1]\n",
    "\n",
    "stacked_lons_test = np.transpose(np.array(X_test), axes=(2,0,1))[0]\n",
    "stacked_lats_test = np.transpose(np.array(X_test), axes=(2,0,1))[1]\n",
    "\n",
    "lons_SGD = LinearRegression().fit(stacked_lons_train, lons_train)\n",
    "lats_SGD = LinearRegression().fit(stacked_lats_train, lats_train)\n",
    "\n",
    "lons_pred = lons_SGD.predict(stacked_lons_test)\n",
    "lats_pred = lats_SGD.predict(stacked_lats_test)\n",
    "\n",
    "predictions = np.column_stack((lons_pred, lats_pred))\n",
    "\n",
    "print traj_score(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.640639, -8.615799, -8.630658, ..., -8.574417, -8.64297 ,\n",
       "       -8.578422])"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lats.itemset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stack_col_names = ['kNN_regr', 'CART_regr', 'RF_regr', 'Ada_regr', 'XG_regr', 'Traj_RF', 'Traj_tree']\n",
    "\n",
    "stacked_lons = np.transpose(np.array(stack_X), axes=(2,0,1))[0]\n",
    "stacked_lats = np.transpose(np.array(stack_X), axes=(2,0,1))[1]\n",
    "\n",
    "stacked_lons = pd.DataFrame(data = stacked_lons.T, columns=stack_col_names)\n",
    "stacked_lats = pd.DataFrame(data = stacked_lats.T, columns=stack_col_names)\n",
    "\n",
    "stacked_lons = pd.concat([X, stacked_lons], axis=1)\n",
    "stacked_lats = pd.concat([X, stacked_lats], axis=1)\n",
    "\n",
    "y_lons = np.array(y)[:,0]\n",
    "y_lats = np.array(y)[:,1]\n",
    "\n",
    "X_lons_train, X_lons_test, y_lons_train, y_lons_test = train_test_split(stacked_lons, y_lons, test_size=0.30, random_state = 0)\n",
    "\n",
    "X_lats_train = stacked_lats.ix[X_lons_train.index]\n",
    "X_lats_test = stacked_lats.ix[X_lons_test.index]\n",
    "y_lats_train = np.array([y_lats[idx] for idx in X_lons_train.index])\n",
    "y_lats_test = np.array([y_lats[idx] for idx in X_lons_test.index])\n",
    "\n",
    "lons_LM = LinearRegression().fit(X_lons_train.drop('Traj_tree', 1), y_lons_train)\n",
    "lats_LM = LinearRegression().fit(X_lats_train.drop('Traj_tree', 1), y_lats_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.131444658026\n",
      "-0.0970478037348\n"
     ]
    }
   ],
   "source": [
    "print lons_LM.score(X_lons_test.drop('Traj_tree', 1), y_lons_test)\n",
    "print lats_LM.score(X_lats_test.drop('Traj_tree', 1), y_lats_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = np.column_stack((y_lons_test, y_lats_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.598708, -8.578179, -8.616303, ..., -8.615718, -8.620587,\n",
       "       -8.596188])"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lons_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.4635370564\n"
     ]
    }
   ],
   "source": [
    "lons_hat = lons_LM.predict(X_lons_test.drop('Traj_tree', 1))\n",
    "lats_hat = lats_LM.predict(X_lats_test.drop('Traj_tree', 1))\n",
    "\n",
    "predictions = np.column_stack((lons_hat, lats_hat))\n",
    "\n",
    "print traj_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6914939679\n"
     ]
    }
   ],
   "source": [
    "lm_test = MultiOutputRegressor(LinearRegression()).fit(X_train, y_train)\n",
    "print traj_score(y_test, kNN_regr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to upload to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_trajs = pd.read_csv('test.csv',\n",
    "                        #index_col=,\n",
    "                        #usecols = ['TRIP_ID', 'POLYLINE'],\n",
    "                        converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "# add start_lat and start_lon to the dataset\n",
    "traj = np.array(test_trajs['POLYLINE'])\n",
    "test_trajs['Start_lon'] = [i[0][0] for i in traj]\n",
    "test_trajs['Start_lat'] = [i[0][1] for i in traj]\n",
    "\n",
    "# fill NA values as specific class\n",
    "test_trajs = test_trajs.fillna('0')\n",
    "\n",
    "# make some columns categorical (str)\n",
    "cat_columns = ['CALL_TYPE','DAY_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID']\n",
    "test_trajs[cat_columns] = test_trajs[cat_columns].astype(str)\n",
    "\n",
    "# crate data for clusterization\n",
    "submission = test_trajs['TRIP_ID']\n",
    "test_trajs = test_trajs.drop(['MISSING_DATA', 'TRIP_ID', 'POLYLINE', 'DAY_TYPE'], 1)\n",
    "\n",
    "# Work with timestamps\n",
    "\n",
    "# get list of holidays in Portugal\n",
    "pd_holidays = pd.read_csv('Portugal_holidays.csv')\n",
    "holidays = [date(pd_holidays.Year[i], pd_holidays.Month[i], pd_holidays.Day[i]) for i in range(len(pd_holidays))]\n",
    "del pd_holidays\n",
    "\n",
    "# what i want to get?\n",
    "# year, month, day, seconds from midnight, weeknumber, weekday\n",
    "\n",
    "dates = []\n",
    "for tmstmp in test_trajs['TIMESTAMP']:\n",
    "    t = datetime.utcfromtimestamp(tmstmp)\n",
    "    date_info = []\n",
    "    date_info.append(t.year) # year\n",
    "    date_info.append(t.month) # month\n",
    "    date_info.append(t.isocalendar()[1]) # weeknumber\n",
    "    date_info.append(t.weekday()) # weekday where Monday is 0 and Sunday is 6\n",
    "    date_info.append((t.hour * 3600) + (t.minute * 60) + t.second + (t.microsecond / 1000000.0)) # seconds from midnight\n",
    "    \n",
    "    t_date = t.date()\n",
    "    if t_date in holidays:\n",
    "        date_info.append('B')\n",
    "    elif (date_info[3] == 6) or (date_info[3] == 5):\n",
    "        date_info.append('B')\n",
    "    elif (t_date + timedelta(1)) in holidays:\n",
    "        date_info.append('C')\n",
    "    elif date_info[3] == 4:\n",
    "        date_info.append('C')\n",
    "    else:\n",
    "        date_info.append('A')\n",
    "    \n",
    "    dates.append(date_info)\n",
    "\n",
    "dates = np.array(dates)\n",
    "dates_columns = ['YEAR', 'MONTH', 'WEEKNUM', 'WEEKDAY', 'SECFROMMID', 'DAY_TYPE']\n",
    "dates_df = pd.DataFrame(data = dates, columns = dates_columns)\n",
    "\n",
    "test_trajs = pd.concat([test_trajs, dates_df], axis=1)\n",
    "test_trajs['SECFROMMID'] = test_trajs['SECFROMMID'].astype(float)\n",
    "\n",
    "dummy_cols = ['CALL_TYPE','DAY_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID', \n",
    "              'MONTH', 'WEEKNUM', 'WEEKDAY']\n",
    "\n",
    "test_trajs = pd.get_dummies(test_trajs, columns=dummy_cols)\n",
    "\n",
    "drop_columns = ['CALL_TYPE_A', 'DAY_TYPE_A', 'ORIGIN_CALL_2002.0', 'ORIGIN_STAND_1.0', 'TAXI_ID_20000542',\n",
    "                'MONTH_8', 'WEEKNUM_33', 'WEEKDAY_0']\n",
    "test_trajs = test_trajs.drop(drop_columns,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_data = np.zeros((len(test_trajs), len(X.columns)))\n",
    "df_test = pd.DataFrame(zero_data, columns = X.columns)\n",
    "\n",
    "for col_name in test_trajs.columns:\n",
    "    if col_name in df_test.columns:\n",
    "        df_test[col_name] = test_trajs[col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_test = stacking(models, df_test)\n",
    "stack_test.append(forest.predict(traj))\n",
    "stack_test.append(tree.predict(traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_stack_test = (np.array(stack_test[:6]))\n",
    "norm_stack_test = np.transpose(norm_stack_test, axes=(1,0,2))\n",
    "\n",
    "stacked_lons_test = np.transpose(np.array(norm_stack_test), axes=(2,0,1))[0]\n",
    "stacked_lats_test = np.transpose(np.array(norm_stack_test), axes=(2,0,1))[1]\n",
    "\n",
    "lons_pred = lons_SGD.predict(stacked_lons_test)\n",
    "lats_pred = lats_SGD.predict(stacked_lats_test)\n",
    "\n",
    "predictions = np.column_stack((lons_pred, lats_pred))\n",
    "\n",
    "submission = pd.DataFrame(data = submission)\n",
    "submission['LATITUDE'] = lats_pred\n",
    "submission['LONGITUDE'] = lons_pred\n",
    "submission.to_csv('stacking.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
